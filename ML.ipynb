{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb47bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901e579",
   "metadata": {},
   "source": [
    "## DATA LOADING, CLEANING & INTEGRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9e929b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Orders.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m orders = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOrders.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlatin-1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m leadtime = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mLeadtimeService.csv\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mlatin-1\u001b[39m\u001b[33m\"\u001b[39m, sep=\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m airports = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mairports.csv\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mlatin-1\u001b[39m\u001b[33m\"\u001b[39m, sep=\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m, decimal =\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marle\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marle\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marle\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marle\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marle\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Orders.csv'"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "\n",
    "orders = pd.read_csv(\"Orders.csv\", encoding=\"latin-1\", sep=\";\", decimal =\",\")\n",
    "leadtime = pd.read_csv(\"LeadtimeService.csv\", encoding=\"latin-1\", sep=\";\")\n",
    "airports = pd.read_csv(\"airports.csv\", encoding=\"latin-1\", sep=\",\", decimal =\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for analysis from orders dataset\n",
    "\n",
    "cols_keep = [\n",
    "    \"Type\",\n",
    "    \"Direction\",\n",
    "    \"DSV-CW Ref.\",\n",
    "    \"Goods Description\",\n",
    "    \"Pcs\",\n",
    "    \"Gwgt\",\n",
    "    \"Cwgt\",\n",
    "    \"NOTIFICATION date & Time\",\n",
    "    \"ACTUAL Delivery & Time\",\n",
    "    \"Service Level\",\n",
    "    \"DGR\",\n",
    "    \"Real Origin Airport\",\n",
    "    \"Real Destination Airport\",\n",
    "    \"ZONE\",\n",
    "    \"AIRPORT ORIGIN-DESTINATION\"\n",
    "]\n",
    "\n",
    "df_analysis = orders[cols_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c34f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis[\"Service Level\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with service level: \"ERROR en Service Level (columna AK)\"\n",
    "\n",
    "df_analysis = df_analysis[df_analysis[\"Service Level\"] != \"ERROR en Service Level (columna AK)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis[\"Service Level\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service Level mapping used in the analysis\n",
    "service_level_mapping = {\n",
    "    \"ROV\": \"Normal\",\n",
    "    \"CRV\": \"Urgent\",\n",
    "    \"AOV\": \"Critical\"\n",
    "}\n",
    "\n",
    "df_analysis[\"Service Type\"] = (\n",
    "    df_analysis[\"Service Level\"]\n",
    "    .map(service_level_mapping)\n",
    ")\n",
    "\n",
    "df_analysis[[\"Service Level\", \"Service Type\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9946823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis[\"ZONE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e17771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "leadtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3325bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop spaces in column names\n",
    "df_analysis.columns = df_analysis.columns.str.strip()\n",
    "leadtime.columns = leadtime.columns.str.strip()\n",
    "\n",
    "# Clean string columns to ensure proper merging\n",
    "df_analysis[\"ZONE\"] = df_analysis[\"ZONE\"].astype(str).str.strip()\n",
    "df_analysis[\"Service Level\"] = df_analysis[\"Service Level\"].astype(str).str.strip()\n",
    "\n",
    "leadtime[\"Zone\"] = leadtime[\"Zone\"].astype(str).str.strip()\n",
    "leadtime[\"SLA\"] = leadtime[\"SLA\"].astype(str).str.strip()\n",
    "\n",
    "# Merge lead time information into the main dataframe\n",
    "\n",
    "df_analysis = df_analysis.merge(\n",
    "    leadtime[[\"Zone\", \"SLA\", \"Leadtime\"]],\n",
    "    how=\"left\",\n",
    "    left_on=[\"ZONE\", \"Service Level\"],\n",
    "    right_on=[\"Zone\", \"SLA\"]\n",
    ")\n",
    "\n",
    "# Limpiar columnas auxiliares del join\n",
    "df_analysis.drop(columns=[\"Zone\", \"SLA\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ab386",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop spaces in column names\n",
    "airports.columns = airports.columns.str.strip()\n",
    "\n",
    "# Clean string columns to ensure proper merging\n",
    "df_analysis[\"Real Origin Airport\"] = df_analysis[\"Real Origin Airport\"].astype(str).str.strip()\n",
    "df_analysis[\"Real Destination Airport\"] = df_analysis[\"Real Destination Airport\"].astype(str).str.strip()\n",
    "\n",
    "airports[\"iata_code\"] = airports[\"iata_code\"].astype(str).str.strip()\n",
    "\n",
    "# Merge information into the main dataframe for ORIGIN airport\n",
    "df_analysis = df_analysis.merge(\n",
    "    airports[[\"iata_code\", \"iso_country\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"Real Origin Airport\",\n",
    "    right_on=\"iata_code\"\n",
    ")\n",
    "\n",
    "# Rename columns to avoid confusion\n",
    "df_analysis.rename(\n",
    "    columns={\n",
    "        \"iata_code\": \"iata_code_origin\",\n",
    "        \"iso_country\": \"iso_country_origin\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Merge information into the main dataframe for DESTINATION airport\n",
    "df_analysis = df_analysis.merge(\n",
    "    airports[[\"iata_code\", \"iso_country\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"Real Destination Airport\",\n",
    "    right_on=\"iata_code\"\n",
    ")\n",
    "\n",
    "# Rename columns to avoid confusion\n",
    "df_analysis.rename(\n",
    "    columns={\n",
    "        \"iata_code\": \"iata_code_destination\",\n",
    "        \"iso_country\": \"iso_country_destination\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fe18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataframe for analysis\n",
    "df_analysis = df_analysis.drop(columns=[\"Real Origin Airport\", \"Real Destination Airport\"])\n",
    "df_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7a039",
   "metadata": {},
   "source": [
    "## TARGET ENGINEERING (leadtime_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a6b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert date columns to datetime format\n",
    "\n",
    "notification_col = \"NOTIFICATION date & Time\"\n",
    "delivery_col = \"ACTUAL Delivery & Time\"\n",
    "\n",
    "df_analysis[notification_col] = pd.to_datetime(\n",
    "    df_analysis[notification_col],\n",
    "    errors=\"coerce\",\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "df_analysis[delivery_col] = pd.to_datetime(\n",
    "    df_analysis[delivery_col],\n",
    "    errors=\"coerce\",\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "print(\"\\nMissing timestamps after datetime conversion:\")\n",
    "display(df_analysis[[notification_col, delivery_col]].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with missing timestamps\n",
    "df_analysis = df_analysis.dropna(subset=[notification_col, delivery_col])\n",
    "\n",
    "df_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating leadtime real in hours\n",
    "\n",
    "df_analysis[\"leadtime_real_hours\"] = (\n",
    "    (df_analysis[delivery_col] - df_analysis[notification_col])\n",
    "    .dt.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "df_analysis[[\"leadtime_real_hours\", notification_col, delivery_col]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd284dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative lead times check because of data errors\n",
    "print(\"Negatives:\", (df_analysis[\"leadtime_real_hours\"] < 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09308f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with negative and 0 lead times\n",
    "\n",
    "negatives_df = df_analysis[df_analysis[\"leadtime_real_hours\"] <= 0]\n",
    "\n",
    "print(f\"Negative lead times: {len(negatives_df)}\")\n",
    "\n",
    "display(\n",
    "    negatives_df[\n",
    "        [\n",
    "            \"NOTIFICATION date & Time\",\n",
    "            \"ACTUAL Delivery & Time\",\n",
    "            \"leadtime_real_hours\",\n",
    "            \"ZONE\",\n",
    "            \"Service Level\",\n",
    "            \"Service Type\"\n",
    "        ]\n",
    "    ].sort_values(\"leadtime_real_hours\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23063d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flagging invalid lead times\n",
    "\n",
    "df_analysis[\"invalid_leadtime_flag\"] = (df_analysis[\"leadtime_real_hours\"] <= 0).astype(int)\n",
    "\n",
    "df_analysis[\"invalid_leadtime_flag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed958915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis[df_analysis[\"leadtime_real_hours\"] > 0].copy()\n",
    "\n",
    "#Statistics summary\n",
    "print(df_analysis[\"leadtime_real_hours\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c86c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a target with 99th percentile lead time for avoid outliers\n",
    "\n",
    "leadtime_99th_percentile = df_analysis[\"leadtime_real_hours\"].quantile(0.99)\n",
    "\n",
    "df_analysis[\"leadtime_ml_hours\"] = df_analysis[\"leadtime_real_hours\"].clip(upper=leadtime_99th_percentile)\n",
    "\n",
    "#Statistics summary\n",
    "print(df_analysis[\"leadtime_ml_hours\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a264815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leadtime_real_hours: real operational lead time (EDA)\n",
    "# leadtime_ml_hours: capped target used for ML training with 99th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cfe69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating on_time comparing leadtime_real_hours with SLA\n",
    "df_analysis[\"on_time\"] = (df_analysis[\"leadtime_real_hours\"] <= df_analysis[\"Leadtime\"]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cc605",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea803fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLA RECOMMENDATION (Business logic)\n",
    "\n",
    "# Goal: downgrade ONLY as needed based on real leadtime:\n",
    "# - If real <= SLA(Critical)  -> recommend Critical\n",
    "# - elif real <= SLA(Urgent)  -> recommend Urgent\n",
    "# - else                      -> Normal   (fallback if nothing reaches)\n",
    "\n",
    "# IMPORTANT: This avoids jumping to Normal if Urgent already covers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ede1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking SLA service levels (1=cheapest, 3=most expensive)\n",
    "sla_rank = {\n",
    "    \"Normal\": 1,\n",
    "    \"Urgent\": 2,\n",
    "    \"Critical\": 3\n",
    "}\n",
    "\n",
    "# Create a lookup dictionary for SLA targets by ZONE + Service Type\n",
    "sla_targets_lookup = (\n",
    "    df_analysis\n",
    "    .groupby([\"ZONE\", \"Service Type\"])[\"Leadtime\"]\n",
    "    .first()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Vectorized: create SLA target columns for each service type per zone\n",
    "df_analysis['_sla_critical'] = df_analysis['ZONE'].map(lambda z: sla_targets_lookup.get((z, 'Critical'), float('inf')))\n",
    "df_analysis['_sla_urgent'] = df_analysis['ZONE'].map(lambda z: sla_targets_lookup.get((z, 'Urgent'), float('inf')))\n",
    "df_analysis['_sla_normal'] = df_analysis['ZONE'].map(lambda z: sla_targets_lookup.get((z, 'Normal'), float('inf')))\n",
    "\n",
    "# Step 1: Check if contracted SLA was met (on_time)\n",
    "met_sla = df_analysis['leadtime_real_hours'] <= df_analysis['Leadtime']\n",
    "\n",
    "# Step 2: Find best fit - check from MOST EXPENSIVE to CHEAPEST (Critical → Urgent → Normal)\n",
    "# This ensures we recommend the CLOSEST lower tier, not always Normal\n",
    "\n",
    "conditions_best_fit = [\n",
    "    df_analysis['leadtime_real_hours'] <= df_analysis['_sla_critical'],\n",
    "    df_analysis['leadtime_real_hours'] <= df_analysis['_sla_urgent'],\n",
    "    df_analysis['leadtime_real_hours'] <= df_analysis['_sla_normal']\n",
    "]\n",
    "choices_best_fit = ['Critical', 'Urgent', 'Normal']\n",
    "best_fit = np.select(conditions_best_fit, choices_best_fit, default='Normal')  # Out of SLA → Normal\n",
    "\n",
    "# Step 3: Cap best_fit so it NEVER recommends a more expensive tier than contracted\n",
    "# If delivery was faster than contracted → that's a win, keep contracted level\n",
    "best_fit_rank = pd.Series(best_fit).map(sla_rank).values\n",
    "contracted_rank = df_analysis['Service Type'].map(sla_rank).values\n",
    "best_fit_capped = np.where(best_fit_rank > contracted_rank, df_analysis['Service Type'], best_fit)\n",
    "\n",
    "# Step 4: Final recommendation\n",
    "# - If SLA met → keep contracted level (no change needed)\n",
    "# - If SLA NOT met → use capped best_fit (closest lower tier, never an upgrade)\n",
    "df_analysis['sla_recommended'] = np.where(met_sla, df_analysis['Service Type'], best_fit_capped)\n",
    "\n",
    "# Map to ranks for comparison\n",
    "df_analysis['sla_contracted_rank'] = df_analysis['Service Type'].map(sla_rank)\n",
    "df_analysis['sla_recommended_rank'] = df_analysis['sla_recommended'].map(sla_rank)\n",
    "\n",
    "# Calculate levels to downgrade\n",
    "df_analysis['levels_to_downgrade'] = (\n",
    "    df_analysis['sla_contracted_rank'] - df_analysis['sla_recommended_rank']\n",
    ")\n",
    "\n",
    "# Downgrade is only possible if there are actual savings (levels_to_downgrade > 0)\n",
    "df_analysis['downgrade_possible'] = (df_analysis['levels_to_downgrade'] > 0)\n",
    "\n",
    "# Clean up temp columns\n",
    "df_analysis.drop(['_sla_critical', '_sla_urgent', '_sla_normal'], axis=1, inplace=True)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"DOWNGRADE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal orders analyzed: {len(df_analysis)}\")\n",
    "print(f\"Orders that MET SLA (keep contracted): {met_sla.sum()} ({met_sla.mean():.1%})\")\n",
    "print(f\"Orders with downgrade opportunity: {df_analysis['downgrade_possible'].sum()}\")\n",
    "print(f\"Downgrade rate: {df_analysis['downgrade_possible'].mean():.1%}\")\n",
    "\n",
    "print(\"\\nBreakdown by levels_to_downgrade:\")\n",
    "print(df_analysis['levels_to_downgrade'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nDowngrade opportunities by contracted Service Type:\")\n",
    "downgrade_by_service = df_analysis.groupby(\"Service Type\").agg({\n",
    "    'downgrade_possible': ['sum', 'mean'],\n",
    "    'levels_to_downgrade': 'mean'\n",
    "})\n",
    "print(downgrade_by_service)\n",
    "\n",
    "print(\"\\nRecommended SLA distribution:\")\n",
    "print(pd.crosstab(df_analysis['Service Type'], df_analysis['sla_recommended'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosstab: Contracted Service Type vs Recommended SLA\n",
    "sla_distribution = pd.crosstab(\n",
    "    df_analysis[\"Service Type\"],        # SLA contratado\n",
    "    df_analysis[\"sla_recommended\"],     # SLA recomendado\n",
    "    margins=True,                       # añade totales\n",
    "    margins_name=\"All\"\n",
    ")\n",
    "\n",
    "sla_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe498a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLES:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Downgrade possible (levels > 0):\")\n",
    "print(df_analysis[df_analysis[\"downgrade_possible\"]][\n",
    "    [\"DSV-CW Ref.\", \"ZONE\", \"Service Type\", \"leadtime_real_hours\", \"Leadtime\", \n",
    "     \"sla_recommended\", \"levels_to_downgrade\", \"downgrade_possible\"]\n",
    "].head(3))\n",
    "\n",
    "print(\"\\n2. No downgrade (levels = 0, already at minimum or met SLA):\")\n",
    "print(df_analysis[df_analysis[\"levels_to_downgrade\"] == 0][\n",
    "    [\"DSV-CW Ref.\", \"ZONE\", \"Service Type\", \"leadtime_real_hours\", \"Leadtime\", \n",
    "     \"sla_recommended\", \"levels_to_downgrade\", \"downgrade_possible\"]\n",
    "].head(3))\n",
    "\n",
    "print(\"\\n3. Specific order SBCN0260531:\")\n",
    "specific_order = df_analysis[df_analysis[\"DSV-CW Ref.\"] == \"SBCN0260531\"][\n",
    "    [\"DSV-CW Ref.\", \"ZONE\", \"Service Type\", \"leadtime_real_hours\", \"Leadtime\", \n",
    "     \"sla_recommended\", \"levels_to_downgrade\", \"downgrade_possible\"]\n",
    "]\n",
    "if len(specific_order) > 0:\n",
    "    print(specific_order)\n",
    "else:\n",
    "    print(\"Order not found in filtered dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4aa0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2ad34",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff47e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985359a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Start from df_analysis ONCE\n",
    "df_ml = df_analysis.copy()\n",
    "\n",
    "# 1) Create time features\n",
    "df_ml[\"NOTIFICATION date & Time\"] = pd.to_datetime(\n",
    "    df_ml[\"NOTIFICATION date & Time\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "df_ml[\"day_of_week\"] = df_ml[\"NOTIFICATION date & Time\"].dt.dayofweek\n",
    "df_ml[\"month\"] = df_ml[\"NOTIFICATION date & Time\"].dt.month\n",
    "df_ml[\"hour\"] = df_ml[\"NOTIFICATION date & Time\"].dt.hour\n",
    "\n",
    "# 2) Drop leakage / not-needed columns (ONE single drop)\n",
    "cols_to_drop = [\n",
    "    # Post-delivery / leakage\n",
    "    \"ACTUAL Delivery & Time\",\n",
    "    \"leadtime_real_hours\",\n",
    "    \"leadtime_ml_hours\",\n",
    "    \"invalid_leadtime_flag\",\n",
    "    \"on_time\",\n",
    "\n",
    "    # SLA / contracted / EDA logic (leakage)\n",
    "    \"Service Level\",\n",
    "    \"Service Type\",\n",
    "    \"Leadtime\",\n",
    "    \"sla_contracted_rank\",\n",
    "    \"sla_recommended_rank\",\n",
    "    \"sla_required\",\n",
    "    \"levels_to_downgrade\",\n",
    "    \"downgrade_possible\",\n",
    "\n",
    "    # Identifiers / descriptive\n",
    "    \"Type\",\n",
    "    \"DSV-CW Ref.\",\n",
    "    \"Goods Description\",\n",
    "\n",
    "    # Raw datetime (already engineered)\n",
    "    \"NOTIFICATION date & Time\",\n",
    "]\n",
    "\n",
    "df_ml.drop(columns=[c for c in cols_to_drop if c in df_ml.columns], inplace=True)\n",
    "\n",
    "# 3) Final check\n",
    "df_ml.columns.sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0674181",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aa7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642ab67",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA will be used as an exploratory technique to analyze feature relevance and redundancy.\n",
    "It helps to identify which variables explain most of the variance and which ones are highly correlated or low-impact.\n",
    "The results will guide our feature selection while keeping the final models interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329216a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select numerical features for PCA\n",
    "\n",
    "num_features = df_ml.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "X_num = df_ml[num_features]\n",
    "\n",
    "#PCA pipeline with scaling\n",
    "\n",
    "pca_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA())\n",
    "])\n",
    "\n",
    "X_pca = pca_pipeline.fit_transform(X_num)\n",
    "\n",
    "# Explained variance ratio of the PCA components\n",
    "\n",
    "pca = pca_pipeline.named_steps[\"pca\"]\n",
    "\n",
    "explained_var = pd.DataFrame({\n",
    "    \"PC\": [f\"PC{i+1}\" for i in range(len(pca.explained_variance_ratio_))],\n",
    "    \"Explained Variance Ratio\": pca.explained_variance_ratio_,\n",
    "    \"Cumulative Variance\": np.cumsum(pca.explained_variance_ratio_)\n",
    "})\n",
    "\n",
    "explained_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA loadings to understand feature contributions to each principal component\n",
    "\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    index=num_features,\n",
    "    columns=[f\"PC{i+1}\" for i in range(len(num_features))]\n",
    ")\n",
    "\n",
    "loadings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0736ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall importance of original features based on loadings and explained variance\n",
    "\n",
    "importance = (\n",
    "    loadings.abs()\n",
    "    .mul(pca.explained_variance_ratio_, axis=1)\n",
    "    .sum(axis=1)\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc5449",
   "metadata": {},
   "source": [
    "- PC1 captures shipment physical characteristics, mainly driven by weight-related features\n",
    "- PC2 represents delivery time behavior combining target and real lead time.\n",
    "- Higher-order components explain residual variance and were not considered for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba59910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numenical features for ML model\n",
    "num_features = [\n",
    "    \"Gwgt\",\n",
    "    \"Pcs\"\n",
    "]\n",
    "\n",
    "#catergorical features for ML model\n",
    "cat_features = [\n",
    "    \"ZONE\",\n",
    "    \"DGR\", # Dangerous goods\n",
    "    \"Direction\",\n",
    "    \"AIRPORT ORIGIN-DESTINATION\",\n",
    "]\n",
    "\n",
    "#temporal features for ML model\n",
    "time_features = [\n",
    "    \"day_of_week\",\n",
    "    \"month\",\n",
    "    \"hour\" #order time of day could impact leadtime and SLA performance\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca56623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X and Y for ML model\n",
    "\n",
    "features = num_features + cat_features + time_features\n",
    "\n",
    "X = df_ml[features]\n",
    "y = df_ml[\"sla_recommended\"]  # Using recommended SLA as target for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f646ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify = y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7cf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check target distribution in train and test sets to ensure stratification worked\n",
    "\n",
    "def pct(s):\n",
    "    return (pd.Series(s).value_counts(normalize=True) * 100).round(2)\n",
    "\n",
    "print(\"\\nTarget distribution (%)\")\n",
    "print(\"TRAIN:\\n\", pct(y_train))\n",
    "print(\"\\nTEST:\\n\", pct(y_test))\n",
    "\n",
    "print(\"\\nCounts\")\n",
    "print(\"TRAIN:\\n\", pd.Series(y_train).value_counts())\n",
    "print(\"TEST:\\n\", pd.Series(y_test).value_counts())\n",
    "\n",
    "print(\"\\nSplit sizes\")\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_test :\", X_test.shape, \"y_test :\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build route_grouped using TRAIN only to avoid data leakage\n",
    "\n",
    "route_counts = X_train[\"AIRPORT ORIGIN-DESTINATION\"].value_counts()\n",
    "common_routes = route_counts[route_counts >= 20].index\n",
    "\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "\n",
    "X_train[\"route_grouped\"] = X_train[\"AIRPORT ORIGIN-DESTINATION\"].where(\n",
    "    X_train[\"AIRPORT ORIGIN-DESTINATION\"].isin(common_routes),\n",
    "    \"OTHER\"\n",
    ")\n",
    "\n",
    "X_test[\"route_grouped\"] = X_test[\"AIRPORT ORIGIN-DESTINATION\"].where(\n",
    "    X_test[\"AIRPORT ORIGIN-DESTINATION\"].isin(common_routes),\n",
    "    \"OTHER\"\n",
    ")\n",
    "\n",
    "# Drop original route column after creating route_grouped to avoid data leakage\n",
    "X_train.drop(columns=[\"AIRPORT ORIGIN-DESTINATION\"], inplace=True)\n",
    "X_test.drop(columns=[\"AIRPORT ORIGIN-DESTINATION\"], inplace=True)\n",
    "\n",
    "# Now define final categorical features\n",
    "cat_features = [\"ZONE\", \"DGR\", \"Direction\", \"route_grouped\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipelines for numerical and categorical features\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_features + time_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "         cat_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac7cacb",
   "metadata": {},
   "source": [
    "## MODEL 0 - BASELINE (Dummy)\n",
    "\n",
    "A Dummy Regressor is used as a baseline model to establish a minimum performance benchmark.\n",
    "\n",
    "This model does not learn from the data and always predicts the median historical lead time, regardless of shipment characteristics. Any machine learning model must outperform this baseline to be considered meaningful and add predictive value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb18e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", DummyClassifier(strategy=\"most_frequent\", random_state=42))\n",
    "])\n",
    "\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy_clf.predict(X_test)\n",
    "\n",
    "print(\"=== DummyClassifier (most_frequent) ===\")\n",
    "print(classification_report(y_test, y_pred_dummy))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_dummy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237efabb",
   "metadata": {},
   "source": [
    "## MODEL 1 - LINEAR REGRESSION (Control)\n",
    "\n",
    "In Linear Regression models the relationship between *shipment features* and leadtime are considered as a linear combination of inputs.\n",
    "\n",
    "It serves as a control model to verify whether simple linear patterns can explain delivery times. Its performance provides a reference point before moving to more complex, non-linear models.\n",
    "\n",
    "*In this project, shipment features refer to all operational, physical, geographical and temporal variables available at booking time, including zone, service type, shipment weight, origin and destination countries and booking time characteristics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e3234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\",   # helps if classes are imbalanced\n",
    "        n_jobs=None                # keep default for compatibility\n",
    "    ))\n",
    "])\n",
    "\n",
    "logreg_clf.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg_clf.predict(X_test)\n",
    "\n",
    "print(\"\\n=== LogisticRegression ===\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_logreg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65234fbd",
   "metadata": {},
   "source": [
    "## MODEL 2 - RANDOM FOREST REGRESSOR (Core Model)\n",
    "\n",
    "The Random Forest Regressor combines multiple decision trees to capture non-linear relationships and feature interactions.\n",
    "\n",
    "It is well suited for logistics data, where delivery times depend on complex interactions between zones, service levels and weights. This model is expected to significantly outperform linear approaches in predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0429c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "print(\"\\n=== RandomForestClassifier ===\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aeb8ce",
   "metadata": {},
   "source": [
    "## MODEL 3 - GRADIENT BOOSTING \n",
    "\n",
    "Gradient Boosting builds trees sequentially, each one correcting the errors of the previous model.\n",
    "\n",
    "It focuses on difficult cases and often achieves high predictive performance on structured tabular data.\n",
    "However, it requires careful tuning to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6859ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gb_clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_pred_gb = gb_clf.predict(X_test)\n",
    "\n",
    "print(\"\\n=== GradientBoostingClassifier ===\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_gb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02974b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "models = {\n",
    "    \"Dummy\": dummy_clf,\n",
    "    \"Logistic Regression\": logreg_clf,\n",
    "    \"Random Forest\": rf_clf,\n",
    "    \"Gradient Boosting\": gb_clf\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Macro Recall\": recall_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"Recall Normal\": recall_score(y_test, y_pred, labels=[\"Normal\"], average=None)[0],\n",
    "        \"Recall Urgent\": recall_score(y_test, y_pred, labels=[\"Urgent\"], average=None)[0],\n",
    "        \"Recall Critical\": recall_score(y_test, y_pred, labels=[\"Critical\"], average=None)[0],\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index(\"Model\").round(3)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bdbc0",
   "metadata": {},
   "source": [
    "Several models were tested. Gradient Boosting showed the best trade-off between overall performance and recall for minority classes, while remaining stable and interpretable. Therefore, it was selected as the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d6453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFUSION MATRIX FOR FINAL MODEL (Gradient Boosting)\n",
    "\n",
    "y_pred_final = y_pred_gb # Gradient Boosting is the final chosen model\n",
    "\n",
    "labels = [\"Normal\", \"Urgent\", \"Critical\"]\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_final, labels=labels)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cm)\n",
    "plt.title(\"Confusion Matrix – Final Model\")\n",
    "plt.xlabel(\"Predicted SLA\")\n",
    "plt.ylabel(\"Actual SLA\")\n",
    "plt.xticks(range(len(labels)), labels)\n",
    "plt.yticks(range(len(labels)), labels)\n",
    "\n",
    "# Annotate cells with counts\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd2ac7",
   "metadata": {},
   "source": [
    "The confusion matrix shows strong performance for Normal shipments, which represent the majority of the data.\n",
    "Urgent and especially Critical shipments remain challenging due to strong class imbalance, with the model tending to under-predict higher SLA levels.\n",
    "This behavior reflects a conservative recommendation strategy, prioritizing cost efficiency over minority class recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c0b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SLA distribution in actual vs predicted to check if model is capturing the overall class distribution\n",
    "\n",
    "df_dist = pd.DataFrame({\n",
    "    \"Actual\": pd.Series(y_test).value_counts(normalize=True),\n",
    "    \"Predicted\": pd.Series(y_pred_final).value_counts(normalize=True)\n",
    "}).fillna(0)\n",
    "\n",
    "df_dist = df_dist.loc[[\"Normal\", \"Urgent\", \"Critical\"]]\n",
    "\n",
    "df_dist.plot(kind=\"bar\", figsize=(7, 5))\n",
    "plt.title(\"SLA Distribution – Actual vs Predicted\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.xlabel(\"SLA Level\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
